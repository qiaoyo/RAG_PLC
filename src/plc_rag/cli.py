"""Typer-based CLI to run ingestion, indexing, querying, and fine-tuning."""

from __future__ import annotations

from pathlib import Path
from typing import Optional

import typer

from .chunkers import TextChunker
from .config import (
    CorpusConfig,
    EmbeddingConfig,
    EmbeddingFineTuneConfig,
    GeneratorConfig,
    LoraFineTuneConfig,
    RagRuntimeConfig,
    VectorStoreConfig,
)
from .datasets import (
    load_books_corpus,
    load_chunks,
    load_instruction_corpus,
    save_chunks,
)
from .embedding_finetune import train_embedding_model
from .embeddings import EmbeddingBackend
from .finetune import run_lora_finetune
from .rag_pipeline import LLMGenerator, RagComponents, RagPipeline, PromptBuilder
from .utils import setup_logging
from .vector_store import FaissVectorStore

app = typer.Typer(help="PLC RAG builder CLI")


@app.command()
def ingest(
    code_file: Path = typer.Option(
        Path("data/data_sample/train_001.json"),
        help="Path to PLC instruction JSON dataset.",
    ),
    books_dir: Path = typer.Option(Path("books"), help="Directory containing reference PDFs."),
    chunk_size: int = typer.Option(800, help="Chunk size in words."),
    chunk_overlap: int = typer.Option(120, help="Overlap in words between chunks."),
    min_chunk_size: int = typer.Option(200, help="Minimum chunk size to keep."),
    limit_records: Optional[int] = typer.Option(None, help="Optional limit on instruction records."),
    limit_book_pages: Optional[int] = typer.Option(
        None, help="Limit number of pages per PDF during ingestion."
    ),
    output_file: Path = typer.Option(
        Path("data/processed/chunks.jsonl"), help="Destination JSONL file for chunks."
    ),
) -> None:
    """Load instructions + books, chunk them, and write JSONL for indexing."""

    setup_logging()
    corpus_config = CorpusConfig(
        instruction_corpus=code_file,
        books_dir=books_dir,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        min_chunk_size=min_chunk_size,
        limit_records=limit_records,
        limit_book_pages=limit_book_pages,
        output_chunks_file=output_file,
    )

    instruction_docs = load_instruction_corpus(
        corpus_config.instruction_corpus, limit=corpus_config.limit_records
    )
    book_docs = load_books_corpus(
        corpus_config.books_dir, limit_pages=corpus_config.limit_book_pages
    )
    documents = instruction_docs + book_docs
    if not documents:
        raise typer.BadParameter("No documents found to chunk. Check dataset paths.")

    chunker = TextChunker(
        chunk_size=corpus_config.chunk_size,
        chunk_overlap=corpus_config.chunk_overlap,
        min_chunk_size=corpus_config.min_chunk_size,
    )
    chunks = chunker.split_corpus(documents)
    save_chunks(chunks, corpus_config.output_chunks_file)
    typer.echo(f"Saved {len(chunks)} chunks to {corpus_config.output_chunks_file}")


@app.command()
def index(
    chunks_file: Path = typer.Option(
        Path("data/processed/chunks.jsonl"), help="Chunk JSONL generated by ingest."
    ),
    vector_store_dir: Path = typer.Option(Path("data/vector_store"), help="Index directory."),
    embedding_model: str = typer.Option(
        "BAAI/bge-large-zh-v1.5", help="Sentence-transformer embedding model."
    ),
    batch_size: int = typer.Option(32, help="Embedding batch size."),
    device: Optional[str] = typer.Option(None, help="Override embedding device (cpu/cuda)."),
    recreate: bool = typer.Option(False, help="Overwrite existing FAISS index."),
) -> None:
    """Build FAISS index from chunked documents."""

    setup_logging()
    chunks = load_chunks(chunks_file)
    if not chunks:
        raise typer.BadParameter(f"No chunks found in {chunks_file}")

    embed_config = EmbeddingConfig(
        model_name=embedding_model,
        batch_size=batch_size,
        device=device,
    )
    embedder = EmbeddingBackend(embed_config)
    embeddings = embedder.embed_chunks(chunks)

    vector_config = VectorStoreConfig(storage_dir=vector_store_dir)
    store = FaissVectorStore(vector_config, dim=embeddings.shape[1], recreate=recreate)
    store.add(embeddings, chunks)
    store.save()
    typer.echo(f"Indexed {len(chunks)} chunks into {vector_store_dir}")


@app.command()
def query(
    question: str = typer.Argument(..., help="Question to run through the RAG pipeline."),
    vector_store_dir: Path = typer.Option(Path("data/vector_store"), help="Index directory."),
    top_k: int = typer.Option(5, help="Number of retrieved chunks."),
    embedding_model: str = typer.Option(
        "BAAI/bge-large-zh-v1.5", help="Embedding model used during indexing."
    ),
    llm: str = typer.Option("Qwen/Qwen2.5-7B-Instruct", help="LLM for generation."),
    max_new_tokens: int = typer.Option(512, help="Max new tokens for generation."),
    temperature: float = typer.Option(0.2, help="Sampling temperature."),
) -> None:
    """Query the local FAISS index and generate an answer."""

    setup_logging()
    rag_config = RagRuntimeConfig(
        embedding=EmbeddingConfig(model_name=embedding_model),
        vector_store=VectorStoreConfig(storage_dir=vector_store_dir, top_k=top_k),
        generator=GeneratorConfig(model_name=llm, max_new_tokens=max_new_tokens, temperature=temperature),
    )

    embedder = EmbeddingBackend(rag_config.embedding)
    store = FaissVectorStore(rag_config.vector_store)
    generator = LLMGenerator(rag_config.generator)
    prompt_builder = PromptBuilder()
    pipeline = RagPipeline(
        RagComponents(
            embedder=embedder,
            vector_store=store,
            generator=generator,
            prompt_builder=prompt_builder,
        ),
        rag_config=rag_config,
    )

    result = pipeline.run(question)
    typer.echo("=== 答案 ===")
    typer.echo(result["answer"])
    typer.echo("\n=== 参考上下文 ===")
    for hit in result["context"]:
        typer.echo(f"- {hit.metadata.get('source')} (score={hit.score:.3f})")


@app.command("finetune-llm")
def finetune_llm(
    dataset_path: Path = typer.Option(
        Path("data/data_sample/train_001.json"), help="Training dataset JSON."
    ),
    base_model: str = typer.Option("Qwen/Qwen2.5-7B-Instruct", help="Base LLM checkpoint."),
    output_dir: Path = typer.Option(Path("artifacts/llm_lora"), help="Where to save LoRA weights."),
    micro_batch_size: int = typer.Option(2, help="Per-device batch size."),
    grad_accum: int = typer.Option(8, help="Gradient accumulation steps."),
    num_epochs: int = typer.Option(3, help="Number of fine-tuning epochs."),
    learning_rate: float = typer.Option(2e-4, help="Learning rate."),
) -> None:
    """LoRA fine-tune the base LLM on PLC instructions."""

    setup_logging()
    config = LoraFineTuneConfig(
        base_model=base_model,
        dataset_path=dataset_path,
        output_dir=output_dir,
        micro_batch_size=micro_batch_size,
        gradient_accumulation_steps=grad_accum,
        num_epochs=num_epochs,
        learning_rate=learning_rate,
    )
    run_lora_finetune(config)


@app.command("finetune-embedding")
def finetune_embedding(
    dataset_path: Path = typer.Option(
        Path("data/data_sample/train_001.json"), help="Training dataset JSON."
    ),
    base_model: str = typer.Option("BAAI/bge-large-zh-v1.5", help="Base embedding model."),
    output_dir: Path = typer.Option(
        Path("artifacts/embedding_model"), help="Directory to save fine-tuned embeddings."
    ),
    batch_size: int = typer.Option(32, help="Batch size for embedding fine-tuning."),
    num_epochs: int = typer.Option(2, help="Number of epochs."),
    learning_rate: float = typer.Option(2e-5, help="Learning rate."),
) -> None:
    """Fine-tune the embedding model using cosine similarity loss."""

    setup_logging()
    config = EmbeddingFineTuneConfig(
        base_model=base_model,
        dataset_path=dataset_path,
        output_dir=output_dir,
        batch_size=batch_size,
        num_epochs=num_epochs,
        learning_rate=learning_rate,
    )
    train_embedding_model(config)


def run() -> None:
    app()


if __name__ == "__main__":
    run()
